# -*- coding: utf-8 -*-
"""Untitled55.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kKSgek1JgTYzbzCA7c-tCXePdRuCPZ2d
"""

import os
file_names = ["ch1.txt", "ch2.txt", "ch3.txt", "ch4.txt", "ch5.txt"]
text_data = {}
for file in file_names:
    with open(file, "r", encoding="utf-8") as f:
        text_data[file] = f.read()
for file, content in text_data.items():
    print(f"--- {file} ---\n{content[:500]}\n")



import re
import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans("", "", string.punctuation))
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]
    cleaned_tokens = [word for word in lemmatized_tokens if len(word) > 2]

    return " ".join(cleaned_tokens)

preprocessed_texts = {file: preprocess_text(content) for file, content in text_data.items()}

for file, text in preprocessed_texts.items():
    print(f"--- Processed {file} ---\n{text[:500]}\n")

import spacy
nlp = spacy.load("en_core_web_sm")
def named_entity_analysis(text):
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    return entities

ner_results = {file: named_entity_analysis(text) for file, text in preprocessed_texts.items()}
for file, entities in ner_results.items():
    print(f"--- Named Entities in {file} ---")
    for entity, label in entities:
        print(f"{entity}: {label}")
    print("\n")

import glob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
def load_text_files():
    text_files = glob.glob("*.txt")
    texts = {}
    for file in text_files:
        with open(file, "r", encoding="utf-8") as f:
            text = f.read().replace("\n", " ").strip()
            texts[file] = text
    return texts

def perform_topic_modeling(texts, num_topics=3, num_words=5):
    vectorizer = TfidfVectorizer(stop_words='english')
    text_matrix = vectorizer.fit_transform(texts.values())
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(text_matrix)

    feature_names = vectorizer.get_feature_names_out()
    topics = {}
    for topic_idx, topic in enumerate(lda.components_):
        topics[f"Topic {topic_idx + 1}"] = [feature_names[i] for i in topic.argsort()[:-num_words - 1:-1]]
    return topics

text_data = load_text_files()

topics = perform_topic_modeling(text_data)

print("Topic Modeling Results:")
for topic, words in topics.items():
    print(f"{topic}: {', '.join(words)}")

import glob
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, PCA
from sklearn.manifold import TSNE
def load_text_files():
    text_files = glob.glob("*.txt")
    texts = {}
    for file in text_files:
        with open(file, "r", encoding="utf-8") as f:
            text = f.read().replace("\n", " ").strip()
            texts[file] = text
    return texts

def perform_topic_modeling(texts, num_topics=3):
    vectorizer = TfidfVectorizer(stop_words='english')
    text_matrix = vectorizer.fit_transform(texts.values())
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    topic_distribution = lda.fit_transform(text_matrix)
    return topic_distribution

def visualize_topics(topic_distribution):

    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(topic_distribution)

    tsne = TSNE(n_components=2, perplexity=3, random_state=42)
    tsne_result = tsne.fit_transform(topic_distribution)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.scatter(pca_result[:, 0], pca_result[:, 1], c='blue', alpha=0.6)
    plt.title("PCA Visualization of Topics")

    plt.subplot(1, 2, 2)
    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c='red', alpha=0.6)
    plt.title("t-SNE Visualization of Topics")

    plt.show()

text_data = load_text_files()

topic_distribution = perform_topic_modeling(text_data)

visualize_topics(topic_distribution)

all_text = "\n".join(text_data.values())

with open("combined_data.txt", "w", encoding="utf-8") as f:
    f.write(all_text)

print("Text from all files has been combined into 'combined_data.txt'.")

pip install transformers datasets torch

from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

dataset = TextDataset(
    tokenizer=tokenizer,
    file_path="combined_data.txt",
    block_size=128
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

training_args = TrainingArguments(
    output_dir="./austen_model",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=500,
    save_total_limit=1,
    prediction_loss_only=True,
    logging_dir='./logs',
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()

trainer.save_model("./austen_model")
tokenizer.save_pretrained("./austen_model")

print("Model training complete. Model saved to './austen_model'.")

from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the fine-tuned model
model = GPT2LMHeadModel.from_pretrained("./austen_model")
tokenizer = GPT2Tokenizer.from_pretrained("./austen_model")

input_prompt = "It is a truth universally acknowledged"

inputs = tokenizer.encode(input_prompt, return_tensors='pt')

outputs = model.generate(
    inputs,
    max_length=150,
    num_return_sequences=1,
    temperature=0.9,
    top_k=50,
    top_p=0.95,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)

model.save_pretrained("/content/fine_tuned_model")
tokenizer.save_pretrained("/content/fine_tuned_model")

!pip install streamlit pyngrok transformers

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import GPT2LMHeadModel, GPT2Tokenizer
# import torch
# 
# model = GPT2LMHeadModel.from_pretrained("/content/fine_tuned_model")
# tokenizer = GPT2Tokenizer.from_pretrained("/content/fine_tuned_model")
# 
# st.title("✍️ Jane Austen Style Generator")
# st.write("Enter a prompt and generate a paragraph in Jane Austen’s style.")
# 
# prompt = st.text_area("Enter your prompt:", "It is a truth universally acknowledged,")
# 
# if st.button("Generate"):
#     input_ids = tokenizer.encode(prompt, return_tensors='pt')
# 
#     with torch.no_grad():
#         output_ids = model.generate(
#             input_ids,
#             max_length=250,
#             temperature=0.9,
#             top_k=50,
#             top_p=0.95,
#             early_stopping=True,
#             no_repeat_ngram_size=2
#         )
#     generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
#     st.markdown("**Generated Text:**")
#     st.write(generated_text)
#

model.save_pretrained("/content/fine_tuned_model")
tokenizer.save_pretrained("/content/fine_tuned_model")

from pyngrok import ngrok

# Kill any existing tunnels
ngrok.kill()

# Add your auth token here (only needs to be done once)
ngrok.set_auth_token("2vlt2TySSwM8MPDry260qEAS4sY_3vMWqNBBPYyaYMxPeFzXz")

# Open a tunnel with HTTP on port 8501
# For ngrok versions 3.0+ , it is best to use the config argument like so:
tunnel = ngrok.connect(addr="8501", proto="http", bind_tls=True)
#or simply:
#tunnel = ngrok.connect(8501, "http")  #Without bind_tls

public_url = tunnel.public_url
print(f"Your app is live at: {public_url}")

# Launch the Streamlit app
!streamlit run app.py &>/content/logs.txt &

!git clone https://github.com/anushka14023/jane-austen-gpt.git
